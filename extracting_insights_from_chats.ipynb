{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2638f07a-aafe-4ead-8667-56faa280a266",
   "metadata": {},
   "source": [
    "# Unveiling the Power of GenAI: Extracting Insights from Chats\n",
    "Notebook developed for a workshop at Women in Tech Summit 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fabd1-ef62-4b59-88ff-448d0d33d127",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179360f0-0fac-4a65-b226-492dd21494bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:03:34.670880Z",
     "start_time": "2024-04-22T11:03:31.211639Z"
    }
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from langdetect import detect\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a787b39-31e2-4100-ac61-d8f70c104f34",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf5ba0-6d9f-4608-98d3-4d24f8f52e02",
   "metadata": {},
   "source": [
    "We use Kaggle dataset called Customer Support on Twitter for this workshop, you'll find it here: https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter.\n",
    "\n",
    "To run the preprocessing part of our notebook, unzip the data and copy it to the data/raw_data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55838e3e90386e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_conv = pd.read_csv(\"data/raw_data/csc_twitter/twcs/twcs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d4263-3435-4152-b40f-ab8b6984f0b1",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb84a4-fe05-43e9-8cdf-c01d76641b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_id(df):\n",
    "    '''\n",
    "    Identify which tweets belong to the same conversation\n",
    "    '''\n",
    "    df['conversation_id'] = df['tweet_id']\n",
    "    whole = df.shape[0]\n",
    "    for i in range(len(df)):\n",
    "        if i % 100000 == 0:\n",
    "            print(\"{} out of {} were preprocessed\".format(i, whole))\n",
    "        prev_tweet = df.loc[i, 'in_response_to_tweet_id']\n",
    "        if not np.isnan(prev_tweet):\n",
    "            df_temp = df[df['tweet_id']==prev_tweet]\n",
    "            if len(df_temp) > 0:\n",
    "                new_conv_id = df_temp['conversation_id'].values[0]\n",
    "                df.loc[i, 'conversation_id'] = new_conv_id\n",
    "\n",
    "\n",
    "def filter_data(df_filtered):\n",
    "    # Remove additinal whitespaces\n",
    "    WHITESPACES_CLEANER = re.compile(r'(\\uFEFF|\\s)+')\n",
    "    df_filtered['text'] = df_filtered['text'].str.replace(WHITESPACES_CLEANER, ' ', regex=True)\n",
    "    \n",
    "    # Align punctuation\n",
    "    WHITESPACES_BEFORE_PUNCTUATION_CLEANER = re.compile(r'(\\w)\\s+([.,;:?!])')\n",
    "    df_filtered['text'] = df_filtered['text'].str.replace(WHITESPACES_BEFORE_PUNCTUATION_CLEANER, r'\\1\\2', regex=True)\n",
    "    \n",
    "    # Filter out empty strings\n",
    "    df_filtered = df_filtered[df_filtered['text']!=\"\"]\n",
    "\n",
    "    # Filter out emoticons\n",
    "    df_filtered_emot = df_filtered['text'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "    df_filtered['text'] = df_filtered_emot\n",
    "\n",
    "    # Filter out one char strings\n",
    "    df_filtered = df_filtered.loc[df_filtered['utterance_size'] > 1, :]\n",
    "    \n",
    "    # Replace web links\n",
    "    LINK_CLEANER = re.compile(r'\\b(?:https?://)\\S+', flags=re.IGNORECASE)\n",
    "    df_filtered['text'] = df_filtered['text'].str.replace(LINK_CLEANER, '[link]', regex=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def detect_lang(x):\n",
    "    '''\n",
    "    Detect the language of the input text\n",
    "    '''\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df607031-c239-4bdc-b57d-b05b702e8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # Transform create_at column to datetime\n",
    "    print(\"Transforming create_at column to datetime\")\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "    # Order data by create date\n",
    "    print(\"Ordering data by create date\")\n",
    "    df.sort_values(by='created_at', ignore_index=True, inplace=True)\n",
    "\n",
    "    # Add conversation_id \n",
    "    print(\"Adding conversation_id\")\n",
    "    add_conv_id(df)\n",
    "\n",
    "    # Add author_type\n",
    "    print(\"Adding author_type\")\n",
    "    df['author_type'] = df['author_id'] .map(lambda x: \"user\" if x.isnumeric() else \"support\")\n",
    "\n",
    "    # Delete links to previous tweets from text body\n",
    "    print(\"Deleting links to previous tweets from text body\")\n",
    "    df_filtered = df.copy(deep=True)\n",
    "    df_filtered['text'] = df_filtered['text'].apply(lambda s: re.sub(\"@\\S+ |@\\S+$\", \"\", s))\n",
    "\n",
    "    print(\"Adding columns related to text length\")\n",
    "    # Add number of characters in the utterance \n",
    "    df_filtered['utterance_size'] = df_filtered['text'].str.len()\n",
    "    # Add number of tokens in the utterance \n",
    "    df_filtered['utterance_tokens_size'] = df_filtered['text'].apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "    print(\"Limiting data to AppleSupport\")\n",
    "    # Limit data to AppleSupport - one company, choosing based on number of records in the data\n",
    "    conversation_id_apple = df_filtered.loc[df_filtered['author_id']=='AppleSupport', 'conversation_id'].unique()\n",
    "    df_filtered = df_filtered[df_filtered['conversation_id'].isin(conversation_id_apple)]\n",
    "    \n",
    "    print(\"Filtering data\")\n",
    "    df_filtered = filter_data(df_filtered)\n",
    "\n",
    "    print(\"Adding information about conversation language\")\n",
    "    # Add language info\n",
    "    df_filtered['utterance_lang'] = df_filtered.loc[:, 'text'].apply(lambda x: detect_lang(x))\n",
    "    conv_lang = df_filtered.groupby(['conversation_id']).apply(lambda x: x.sort_values('utterance_size', ascending=False).iloc[0]['utterance_lang'])\n",
    "    df_conv_lang = conv_lang.to_frame().rename(columns={0: \"conversation_lang\"})\n",
    "    df_filtered = df_filtered.merge(df_conv_lang, left_on='conversation_id', right_index=True)\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c610422-3f92-4385-8816-e6b6c964171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv_filtered = preprocessing(df_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2257c-a573-47b1-9a33-4170bf122a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data to a file\n",
    "# df_conv_filtered.to_csv(\"data/preprocessed_data/apple_support.csv\")\n",
    "# df_conv_filtered.to_parquet(\"data/preprocessed_data/apple_support.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3cea5-d06d-401b-a671-ab182c68919c",
   "metadata": {},
   "source": [
    "### Examples before and after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e36e26-e98d-4e65-95f7-0c4d8dcb2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv = pd.read_csv(\"data/raw_data/csc_twitter/twcs/twcs.csv\")\n",
    "# Transform create_at column to datetime\n",
    "df_conv['created_at'] = pd.to_datetime(df_conv['created_at'], errors='coerce')\n",
    "# Order data by create date\n",
    "df_conv.sort_values(by='created_at', ignore_index=True, inplace=True)\n",
    "\n",
    "df_conv_filtered = pd.read_parquet(\"data/preprocessed_data/apple_support.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedf915422dd66f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# before preprocessing\n",
    "for idx, item in df_conv.loc[df_conv['tweet_id'].isin([363663, 363661, 363662])].iterrows():\n",
    "    print(f\"{item['tweet_id']} | {item['author_id']} | {item['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81de07cf9a97af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# after preprocessing\n",
    "selected_conv_id = 363663\n",
    "for idx, item in df_conv_filtered.loc[df_conv_filtered['conversation_id'] == selected_conv_id, :].iterrows():\n",
    "    print(f\"{item['tweet_id']} | {item['author_id']} | {item['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba8db0dc11ff67",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# before preprocessing\n",
    "for idx, item in df_conv.loc[df_conv['tweet_id'].isin([1700946, 1700945, 1700944, 1700943, 1700941, 1700942])].iterrows():\n",
    "    print(f\"{item['tweet_id']} | {item['author_id']} | {item['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40515de68ab161e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# after preprocessing\n",
    "selected_conv_id = 1700946\n",
    "for idx, item in df_conv_filtered.loc[df_conv_filtered['conversation_id'] == selected_conv_id, :].iterrows():\n",
    "    print(f\"{item['tweet_id']} | {item['author_id']} | {item['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f255dea665b25",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Exploratory Data Analysis on preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d42e0b-79d5-4be2-b927-b399a6abdeeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:03:52.403291Z",
     "start_time": "2024-04-22T11:03:50.475257Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Read data from file\n",
    "# df_conv_filtered = pd.read_parquet(\"data/preprocessed_data/apple_support.parq\")\n",
    "# # Transform create_at column to datetime\n",
    "# df_conv_filtered['created_at'] = pd.to_datetime(df_conv_filtered['created_at'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da876e344bf332b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:03:58.207390Z",
     "start_time": "2024-04-22T11:03:58.168320Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_conv_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6738f87f-ea0b-40da-8562-9fb3c11f5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv_filtered['conversation_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d9f8edd64b6ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Number of conversations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8405fc9b54a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:04:15.116369Z",
     "start_time": "2024-04-22T11:04:08.676771Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_conv_created = df_conv_filtered[['conversation_id', 'created_at']]\n",
    "df_conv_created['created_month']= df_conv_filtered['created_at'].dt.strftime('%Y-%m')\n",
    "df_conv_vs_time = df_conv_created.groupby('created_month', as_index=False)\n",
    "df_conv_vs_time = df_conv_vs_time.agg(conversation_count=pd.NamedAgg(column=\"conversation_id\", aggfunc=\"nunique\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36724d941301b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:53:55.955665Z",
     "start_time": "2024-04-19T12:53:55.819411Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_conv_vs_time[(df_conv_vs_time['created_month']>='2014-01') & (df_conv_vs_time['created_month']<='2024-04')], x='created_month', y='conversation_count',\n",
    "             title=\"Number of conversations over time\",\n",
    "            height=600,\n",
    "            text_auto='.2s'\n",
    "            )\n",
    "fig.update_layout(xaxis_title='Created month',\n",
    "                  yaxis_title='Count')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322078dc3656fc0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Number of tweets per conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce5a8c42e2533c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:55:00.031403Z",
     "start_time": "2024-04-19T12:54:59.833816Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_conv_length = df_conv_filtered.groupby('conversation_id')\n",
    "df_conv_length = df_conv_length.agg(\n",
    "    conversation_length=pd.NamedAgg(column=\"tweet_id\", aggfunc=\"count\"),\n",
    "    conversation_size=pd.NamedAgg(column=\"utterance_size\", aggfunc=\"sum\")\n",
    ")\n",
    "conversation_count_vs_length = (df_conv_length[['conversation_length']].value_counts().to_frame().reset_index()).rename(columns={0: 'count'}).sort_values('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffb5d1ff5d674b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:59:13.622046Z",
     "start_time": "2024-04-19T12:59:13.537717Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    conversation_count_vs_length, \n",
    "    x='conversation_length', \n",
    "    y='count',\n",
    "    title=\"Number of conversations vs. conversation length\",\n",
    "    height=600\n",
    ")\n",
    "fig.update_xaxes(range=[0, 20])\n",
    "fig.update_layout(\n",
    "    xaxis_title='Conversation length',\n",
    "    yaxis_title='Count'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60895ca6-ea05-47ce-a98d-01427a2f3659",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9869c7b-aa28-43a4-a800-f1b2e38e111b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:04:46.391888Z",
     "start_time": "2024-04-22T11:04:46.345376Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select time frames and English language\n",
    "begin_date = '2017-09-01'\n",
    "end_date = '2017-12-31'\n",
    "selection_mask = (df_conv_filtered['created_at'] >= begin_date) & \\\n",
    "                 (df_conv_filtered['created_at'] < end_date) & \\\n",
    "                 (df_conv_filtered['conversation_lang'] == 'en')\n",
    "df_conv_selected = df_conv_filtered[selection_mask]\n",
    "\n",
    "# Add who's talking: user or support to tweet text\n",
    "df_conv_selected['text'] = df_conv_selected['author_type'] + ': ' + df_conv_selected['text']\n",
    "\n",
    "# Join messages into one conversation\n",
    "df_conversations = df_conv_selected.groupby(['conversation_id'], as_index=False).agg(\n",
    "    conversation_length=pd.NamedAgg(column=\"tweet_id\", aggfunc=\"count\"),\n",
    "    conversation_body=pd.NamedAgg(column=\"text\", aggfunc=lambda x: '\\n'.join(x.astype(str))), \n",
    "    conversation_date=pd.NamedAgg(column=\"created_at\", aggfunc=\"first\")\n",
    ")\n",
    "\n",
    "# Count number of tokens (words) in each conversation\n",
    "df_conv_tokens = df_conv_selected[['conversation_id', 'utterance_tokens_size']]\n",
    "df_conv_tokens = df_conv_tokens.groupby(['conversation_id'])['utterance_tokens_size'].sum()\n",
    "df_conv_tokens = df_conv_tokens.to_frame().reset_index().rename(columns={'utterance_tokens_size': 'conv_tokens_count'})\n",
    "\n",
    "# Add info about number of tokens\n",
    "df_conversations = df_conversations.merge(df_conv_tokens, how='left', on='conversation_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa91fa-dd2d-4231-ac86-d7369780d1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:11:21.642082Z",
     "start_time": "2024-04-22T11:11:20.993893Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df_conversations, \n",
    "    x='conv_tokens_count', \n",
    "    title=\"Distribution of number of tokens in conversations\",\n",
    "    height=600\n",
    ")\n",
    "fig.update_xaxes(range=[0, 1500])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648c3105e3cf698",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:17:03.624729Z",
     "start_time": "2024-04-22T11:17:03.512494Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Cut too long conversations - max_tokens is set based on distribution of number of tokens in conversation\n",
    "print(df_conversations[['conv_tokens_count']].quantile(0.99))\n",
    "max_tokens = 198\n",
    "df_conversations['conversation_body'] = df_conversations['conversation_body'].apply(lambda x: ' '.join(str(x).split(' ')[:max_tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efed87-519f-4bc3-8de4-4a43db52ea76",
   "metadata": {},
   "source": [
    "### Remove duplicated conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ea77c-ab86-4a31-af85-c81181a88db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:17:33.311388Z",
     "start_time": "2024-04-22T11:17:33.272609Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All duplicated conversations ('keep=False' marks all duplicates as True)\n",
    "df_duplicated_convs = df_conversations.loc[df_conversations['conversation_body'].duplicated(keep=False), :]\n",
    "\n",
    "# Count duplicates and represent duplicates by the first occurrence\n",
    "df_duplicated_conv_count = df_duplicated_convs.groupby('conversation_body', as_index=False).agg(\n",
    "    conversations_count=pd.NamedAgg(column=\"conversation_id\", aggfunc=\"count\"),\n",
    "    conversation_id=pd.NamedAgg(column=\"conversation_id\", aggfunc=\"first\")\n",
    ")[['conversation_id', 'conversation_body', 'conversations_count']].sort_values('conversations_count', ascending=False)\n",
    "\n",
    "# Define unique conversations\n",
    "df_conv_unique = df_conversations[~df_conversations['conversation_body'].duplicated()].reset_index(drop=True)\n",
    "df_conv_unique['conversation_month']= df_conv_unique['conversation_date'].dt.strftime('%Y-%m')\n",
    "df_conv_unique = df_conv_unique.merge(df_duplicated_conv_count, how='left', on=['conversation_id', 'conversation_body'])\n",
    "df_conv_unique = df_conv_unique.fillna({'conversations_count': 1}).astype({'conversations_count': int})\n",
    "\n",
    "# Save the final data\n",
    "# df_conv_unique.to_parquet(\"data/preprocessed_data/apple_support_training_data.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914c45af7bfb454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T13:08:44.682970Z",
     "start_time": "2024-04-22T13:08:41.379061Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read final data\n",
    "# df_conv_unique = pd.read_parquet(\"data/preprocessed_data/apple_support_training_data.parq\")\n",
    "\n",
    "# Get conversations text as list\n",
    "conversations_data = df_conv_unique['conversation_body'].tolist()\n",
    "\n",
    "# Get timestamps as list\n",
    "timestamps = df_conv_unique['conversation_date'].dt.strftime('%Y-%m').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc8cb9-8049-4393-9b33-c409073c8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of final conversations which will be used for training\n",
    "for i in range(5):\n",
    "    print('_' * 50)\n",
    "    print(conversations_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeb7a04-f95e-446b-b3f1-08fcad8b8167",
   "metadata": {},
   "source": [
    "## Topic modelling using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebbb2c-e78a-43c8-ae46-b000298fb53a",
   "metadata": {},
   "source": [
    "For more info on BERTopic, check out their website: https://maartengr.github.io/BERTopic/algorithm/algorithm.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1a339-1123-4e33-9450-ce30a9e7f484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T13:06:08.755344Z",
     "start_time": "2024-04-22T13:05:39.460796Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech, TextGeneration\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2b4f2-2eda-49f6-83b8-417465497936",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f02bb-052e-4c84-891a-bf11505a07ad",
   "metadata": {},
   "source": [
    "##### Detailed BERTopic pipeline\n",
    "- Step 1 - Embedding documents using a Hugging Face Transformers model\n",
    "- Step 2 - Reducing dimensionality of embeddings using UMAP: https://pair-code.github.io/understanding-umap/\n",
    "- Step 3 - Clustering reduced embeddings into topics using HDBSCAN\n",
    "- Step 4 - Tokenization of topics using CountVectorizer\n",
    "- Step 5 - Weight tokens, create topic representation using C-TF-IDF\n",
    "- Step 6 - (Optional) Fine-tune topic representations using KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23427232-e1f6-4130-95a6-2c2f6860ddc3",
   "metadata": {},
   "source": [
    "##### Calculate embeddings\n",
    "Model used to calculate embeddings: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7204d7-b503-4daa-ba05-a7a03dfa3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "conversations_embeddings = embedding_model.encode(conversations_data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40ca59-d585-4f7b-9d79-d9616e78ccd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T13:14:29.095071Z",
     "start_time": "2024-04-22T13:14:25.970218Z"
    }
   },
   "outputs": [],
   "source": [
    "# embedding_model_gte_base = SentenceTransformer('thenlper/gte-base')\n",
    "# conversations_embeddings_gte_base = embedding_model_gte_base.encode(conversations_data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a39b4-d96d-45d1-8c4c-e4cf4ac5fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to a file\n",
    "# np.save(\"conversations_embeddings\", conversations_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfa23e-897f-4512-9f0a-e64b7e59873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings from a file\n",
    "# conversations_embeddings = np.load(\"conversations_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e4fd4-65f0-496b-81c8-ff20bcefc74c",
   "metadata": {},
   "source": [
    "##### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41da31-a554-456a-8d0c-6136f8fc04b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Stopwords list downloaded from https://github.com/stopwords-iso/stopwords-en/blob/master/stopwords-en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb897e-507d-4355-92e4-faf83902b703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T13:14:51.503811Z",
     "start_time": "2024-04-22T13:14:51.460144Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords_list = pd.read_csv('stopwords-en.txt', header=None).astype(str)[0].tolist()\n",
    "# sorted(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76370fa-c479-494c-8608-6c92193a28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warning:\n",
    "# The current process just got forked. Disabling parallelism to avoid deadlocks... To disable this warning, please explicitly set \n",
    "# TOKENIZERS_PARALLELISM=(true | false)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58575152-4672-4c9f-b08b-b3dc72036f24",
   "metadata": {},
   "source": [
    "Info about hyperparameter tuning: https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdfa09-6043-45ab-8f86-135f74e3082d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T13:16:41.402093Z",
     "start_time": "2024-04-22T13:16:38.260787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "# Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=700, min_samples=50, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "# Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=stopwords_list, min_df=10, ngram_range=(1, 2))\n",
    "# Extract topic words\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "# Fine-tune topic representations\n",
    "## KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "## MMR - Diversify topic representations\n",
    "# mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "## GPT-3.5 - generate human-readable lables\n",
    "# client = openai.OpenAI(api_key=\"\")\n",
    "# prompt = \"\"\"\n",
    "# I have a topic that contains the following documents: \n",
    "# [DOCUMENTS]\n",
    "# The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "# Based on the information above, extract a short but highly descriptive topic label. Make sure it is in the following format:\n",
    "# topic: <topic label>\n",
    "# \"\"\"\n",
    "# openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
    "\n",
    "## All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    # \"MMR\": mmr_model,\n",
    "    # \"OpenAI\": openai_model\n",
    "}\n",
    "\n",
    "## BERTopic model\n",
    "conv_topic_model = BERTopic(\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,           # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                     # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,               # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,         # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                 # Step 5 - Extract topic words\n",
    "  representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic representations\n",
    "    \n",
    "  # Hyperparameters\n",
    "  top_n_words=10, # 10 is default\n",
    "  # nr_topics=\"auto\",\n",
    "  calculate_probabilities=True,\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306d979-4dfd-4fdb-b867-c7b12f7745ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topics, conv_probs = conv_topic_model.fit_transform(conversations_data, conversations_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a0f31-a6a9-4421-92dc-fa37da7cf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.save(\"conv_topic_model.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637778c-c7d3-47d8-aaf4-467faf67ed8d",
   "metadata": {},
   "source": [
    "### BERTopic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359a0e2-8092-4596-88e1-fbd8cefad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_info = conv_topic_model.get_topic_info()\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(conv_topic_info.set_index('Topic')[['Count', 'Name', 'Representation', 'KeyBERT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994fdb2-421e-44a8-a750-8f9beaf3c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(conv_topic_info, x='Name', y='Count',\n",
    "             title=\"Number of conversation related to a given topic\",\n",
    "            height=800,\n",
    "            # text_auto=True\n",
    "            )\n",
    "# fig.update_xaxes(range=[0, 100])\n",
    "fig.update_layout(xaxis_title='Topic 1',\n",
    "                  yaxis_title='Count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e089ec2-82a8-4787-8a98-83f4e2803470",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.visualize_barchart(top_n_topics = 12, n_words = 10, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16cfb37-dc40-4755-925e-9b2766d5102f",
   "metadata": {},
   "source": [
    "### Topic results over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa50199-b248-4323-bc86-80778d16c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topics_over_time = conv_topic_model.topics_over_time(conversations_data, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d861c-1653-4a04-8b8f-7dd4d45f46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_over_time(conversations_data_topics, conv_topic_info):\n",
    "    months = np.sort(conversations_data_topics['conversation_month'].unique())\n",
    "    topics = conv_topic_info['Topic'].values\n",
    "    \n",
    "    topics_over_time = [pd.DataFrame(index=topics)]\n",
    "    for month in months:\n",
    "        col = conversations_data_topics.loc[conversations_data_topics['conversation_month']==month, 'Topic'].value_counts().to_frame()\n",
    "        col = col.rename(columns={'Topic': month})\n",
    "        topics_over_time.append(col)\n",
    "        \n",
    "    topics_over_time_df = pd.concat(topics_over_time, axis='columns').fillna(0).astype(int).transpose().reset_index().rename(columns={'index': 'Month'})\n",
    "    topics_over_time_melt_df = topics_over_time_df.melt(id_vars='Month', var_name=\"Topic\", value_name='Frequency')\n",
    "    topics_over_time_melt_df = topics_over_time_melt_df.merge(conv_topic_info[['Topic', 'Name', 'Representation']], how='left', on='Topic')\n",
    "    \n",
    "    return topics_over_time_melt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8098e7b-fd19-4ba9-8e09-25db87f0747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_data_topics = conv_topic_model.get_document_info(conversations_data).merge(df_conv_unique, how='left', left_index=True, right_index=True)\n",
    "# Saving conversations_data_topics\n",
    "conversations_data_topics.to_parquet(\"data/preprocessed_data/conversations_data_topics.parq\")\n",
    "plot_topics_over_time = topics_over_time(conversations_data_topics, conv_topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd4eb1-5f33-4c74-8a67-155044888e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 12\n",
    "\n",
    "fig = px.line(plot_topics_over_time[plot_topics_over_time['Topic']<top_n], \n",
    "              x='Month', y='Frequency', color='Name',\n",
    "              markers=True,\n",
    "              height=600)\n",
    "fig.update_xaxes(showgrid=True)\n",
    "fig.update_yaxes(showgrid=True)\n",
    "fig.update_layout(legend_traceorder=\"normal\") \n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f\"<b>Topics over Time</b>\",\n",
    "        'y': .95,\n",
    "        'x': 0.40,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top',\n",
    "        'font': dict(\n",
    "            size=22,\n",
    "            color=\"Black\")\n",
    "    },\n",
    "    template=\"simple_white\",\n",
    "    width=1250,\n",
    "    height=600,\n",
    "    hoverlabel=dict(\n",
    "        # bgcolor=\"white\",\n",
    "        font_size=16,\n",
    "        font_family=\"Rockwell\"\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title=\"<b>Global Topic Representation</b>\",\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca79b0-fd33-40e7-a248-12404a385e25",
   "metadata": {},
   "source": [
    "### Representative conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768f3e6-8667-47b1-867e-2a6025b004db",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nb_1 = 0\n",
    "\n",
    "print(conv_topic_info.loc[topic_nb_1 + 1, 'Representation'])\n",
    "print(conv_topic_info.loc[topic_nb_1 + 1, 'Count'])\n",
    "print(\"#\"*50)\n",
    "\n",
    "for item in conv_topic_info.loc[topic_nb_1 + 1, 'Representative_Docs']:\n",
    "    print(item)\n",
    "    print(\"#\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557fdb5-5044-45e1-85ce-6ba7f17119f7",
   "metadata": {},
   "source": [
    "### Topic probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3eec95-dddf-45da-9846-15eb5e7964c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_id = 76849\n",
    "print(conversations_data_topics.loc[selected_id, \"conversation_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033706c-b288-4c6a-ac42-2c2610d083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topic-document distribution for a single document\n",
    "conv_topic_model.visualize_distribution(conv_topic_model.probabilities_[selected_id], custom_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7549d-8dc3-44ca-b202-ada3ead9d5b5",
   "metadata": {},
   "source": [
    "### How similar are documents in different topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca07e07-da45-437b-be0b-1450a2b98a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0c694-4fb9-437c-b6ec-ea8c82bfb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ebb79-c043-4b13-834a-d2a2e4bd4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7adaf-d55e-4157-ad08-001f5f113342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reduce our embeddings to 2D as it will allows us to quickly iterate later on\n",
    "conversations_reduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', \n",
    "                                        random_state=42).fit_transform(conversations_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71566faa-70d4-4f0b-bd79-e33ff461e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.visualize_documents(conversations_data, reduced_embeddings=conversations_reduced_embeddings, \n",
    "                                       hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d62b1c-e149-4513-ad1a-166c62855e77",
   "metadata": {},
   "source": [
    "### Outlier reduction\n",
    "Documentation on outlier reduction: https://maartengr.github.io/BERTopic/getting_started/outlier_reduction/outlier_reduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a333310-a65e-4405-bc7e-cae228402111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of outliers: {conv_topic_info.loc[0, 'Count'] / conv_topic_info['Count'].sum() * 100:.4}%\")\n",
    "print(\"Number of outliers: \", (np.array(conv_topics) == -1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d0a66-23e9-4275-819d-ef07a0390683",
   "metadata": {},
   "source": [
    "The default method for reducing outliers is by calculating the c-TF-IDF representations of outlier documents and assigning them to the best matching c-TF-IDF representations of non-outlier topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afb2d5-c1a9-41df-85b6-6291407bc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the `threshold` parameter to select the minimum distance or similarity when matching outlier documents with non-outlier topics. \n",
    "# This allows the user to change the amount of outlier documents are assigned to non-outlier topics.\n",
    "\n",
    "# Reduce outliers using the `c-tf-idf` strategy\n",
    "new_conv_topics = conv_topic_model.reduce_outliers(conversations_data, conv_topics, strategy=\"c-tf-idf\", threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbb200-947a-47f0-9cce-77ed4506492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of outliers after reduction with c-tf-idf: \", (np.array(new_conv_topics) == -1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c359a7-2c89-4123-b2a8-2feded810359",
   "metadata": {},
   "source": [
    "When outlier documents are generated, they are not used when modeling the topic representations. These documents are completely ignored when finding good descriptions of topics. Hence, after having reduced the number of outliers in your topic model, you might want to update the topic representations with the documents that now belong to actual topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f42c10-f0fb-4c94-bd1b-a6bf45270087",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.update_topics(conversations_data, topics=new_conv_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e8bbf-1774-4c2d-8deb-a578403e3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_topic_model.visualize_documents(conversations_data, reduced_embeddings=conversations_reduced_embeddings, \n",
    "                                       hide_document_hover=True, hide_annotations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea1b7d-1557-4121-b522-ca48a816b7cd",
   "metadata": {},
   "source": [
    "### Other possible improvements \n",
    "#### Outlier reduction - continuation\n",
    "Use the topic distributions, as calculated with `.approximate_distribution` to find the most frequent topic in each outlier document. You can use the `distributions_params` variable to tweak the parameters of `.approximate_distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b257c2a-e7da-433b-8dc4-8b2629a044a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers using the `distributions` strategy\n",
    "# new_conv_topics_d = conv_topic_model.reduce_outliers(conversations_data, conv_topics, strategy=\"distributions\", threshold=0.08)\n",
    "# print(\"Number of outliers after reduction with `distributions` strategy: \", (np.array(new_conv_topics_d) == -1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572975f-6330-4f6c-8807-1aa9e7b0eee7",
   "metadata": {},
   "source": [
    "Probabilities strategy uses the soft-clustering as performed by HDBSCAN to find the best matching topic for each outlier document. To use this, make sure to calculate the `probabilities` beforehand by instantiating BERTopic with `calculate_probabilities=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2be86-aef2-4157-9728-e650aa43cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers using the `probabilities` strategy\n",
    "# new_conv_topics_p = conv_topic_model.reduce_outliers(\n",
    "#     conversations_data, \n",
    "#     conv_topics, \n",
    "#     probabilities=conv_probs, \n",
    "#     strategy=\"probabilities\", \n",
    "#     threshold=0.02\n",
    "# )\n",
    "# print(\"Number of outliers after reduction with `probabilities` strategy: \", (np.array(new_conv_topics_p) == -1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a79ad-3359-46cb-8e28-9568ca5fdc81",
   "metadata": {},
   "source": [
    "Using the embeddings of each outlier documents, find the best matching topic embedding using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f72b3a-81b6-4582-9f97-02d92dc2904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers using the `embeddings` strategy\n",
    "# new_conv_topics_e = conv_topic_model.reduce_outliers(conversations_data, conv_topics, strategy=\"embeddings\", embeddings=conversations_embeddings, threshold=0.5)\n",
    "# print(\"Number of outliers after reduction with `probabilities` strategy: \", (np.array(new_conv_topics_e) == -1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c903b-a05b-489e-84dd-192b6fc8ac0c",
   "metadata": {},
   "source": [
    "#### Topic Reduction after Training\n",
    "Documentation on topic reduction: https://maartengr.github.io/BERTopic/getting_started/topicreduction/topicreduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc1d6a-8029-45f5-a6f2-d9747b618b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.reduce_topics(conversations_data, nr_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa1ff2-f37e-4b20-ba81-6608337f529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26c1db-98da-40b2-8521-d4e48429b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39977b1c-242f-4d51-a221-2e86d24ce891",
   "metadata": {},
   "source": [
    "#### Update Topic Representation after Training\n",
    "Documentation on updating topic representations: https://maartengr.github.io/BERTopic/getting_started/topicrepresentation/topicrepresentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e04c98-bead-401d-b918-6aa5578830fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.update_topics(conversations_data, n_gram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e001a8-7e92-490c-8356-c30ed3ad8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.visualize_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903ee85-1769-410a-bc7c-8f889ad65aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8bfcd-d4d9-4338-9dbe-6be6010c2be3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Extracting labels, main issues and insights using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042a448-9d6f-46c4-8b52-d046d64610c7",
   "metadata": {},
   "source": [
    "## Introduction to Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffc318-82eb-4d48-8a4f-2cdeebc2a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import vertexai\n",
    "from vertexai.preview import generative_models\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594ef46-0586-447a-921c-38df21b23f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(conversations_data_topics: pd.DataFrame, conv_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Formats conversation for a given conversation ID into a string.\n",
    "    \"\"\"\n",
    "    return '- ' + conversations_data_topics.loc[conversations_data_topics['conversation_id']==conv_id, 'Document'].item().replace('\\n', '\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da58237-6bcd-4d2e-8f58-e576a4e2b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_selected_conversations(conversations_data_topics: pd.DataFrame, topic_nb: int, nb_of_convs: int = 10, method: str = 'highest_score') -> str:\n",
    "    \"\"\"\n",
    "    Selects conversations from a given topic number and combines them into one string. By default, it selects 10\n",
    "    conversations using the highest probability score.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    \n",
    "    topic_conversations = conversations_data_topics.loc[conversations_data_topics['Topic']==topic_nb, :].sort_values('Probability', ascending=False)\n",
    "    topic_conversations_probabilities = normalize([topic_conversations['Probability'].values], norm=\"l1\").ravel()\n",
    "    selected_conv_id = []\n",
    "\n",
    "    if method == 'highest_score':\n",
    "        # Select conversations with the highest probability score\n",
    "        selected_conv_id = topic_conversations.iloc[:nb_of_convs]['conversation_id']\n",
    "    elif method == 'score_dist':\n",
    "        # Select conversations based on their probability distribution\n",
    "        selected_conv_id = rng.choice(topic_conversations['conversation_id'], nb_of_convs, replace=False, p=topic_conversations_probabilities)\n",
    "    elif method == 'uniform':\n",
    "        # Select conversations based on uniform distribution over all chats\n",
    "        selected_conv_id = rng.choice(topic_conversations['conversation_id'], nb_of_convs, replace=False)\n",
    "    \n",
    "    conversation_docs_str = \"\"\n",
    "    for nb_of_conv, conv_id in enumerate(selected_conv_id, 1):\n",
    "        conv_body = format_conversation(conversations_data_topics, conv_id) \n",
    "        conversation_docs_str += f'Chat {nb_of_conv}\\n{conv_body}\\n\\n'\n",
    "\n",
    "    return conversation_docs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d785ee1-734d-4876-9b5d-d179be665a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(input_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Deserialize a ``str`` instance containing a JSON document to a Python object.\n",
    "    \"\"\"\n",
    "    json_input = re.sub('(```)?(json)?', '', input_str, flags=re.IGNORECASE)\n",
    "\n",
    "    try:\n",
    "        json_data = json.loads(json_input)\n",
    "    except json.JSONDecodeError as err:\n",
    "        print(f\"Input: {json_input}. This input could not be transformed into a Python object. \"\n",
    "              f\"Error message: {err}\")\n",
    "        json_data = {}\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d1b38-8083-4a28-bc8f-bade1c93c9c4",
   "metadata": {},
   "source": [
    "## Geminni 1.0 Pro (Google LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a178760-30e3-4e49-b3b9-80a705d534e4",
   "metadata": {},
   "source": [
    "**Description**\n",
    "- The best performing model with features for a wide range of text-only tasks.\n",
    "- Supports only text as input.\n",
    "- Supports supervised tuning.\n",
    "\n",
    "**Specifications**\n",
    "- Max total tokens (input and output): **32,760** (A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.)\n",
    "- Max output tokens: **8,192**\n",
    "- Training data: **up to Feb 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b5cde-954a-458f-8b9f-6ea937e172fd",
   "metadata": {},
   "source": [
    "Documentation of Google modles: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc05e7-478e-450c-8b37-5c1e6e2defa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"helix-ds-metal-dev\", location=\"us-central1\")\n",
    "\n",
    "# Load the model\n",
    "gemini_model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "\n",
    "# Generation config\n",
    "gemini_parameters = {\n",
    "    \"max_output_tokens\": 2048, # default: 8192\n",
    "    \"temperature\": 0.5, # default: 0.9\n",
    "    \"top_p\": 0.8, # default: 1\n",
    "}\n",
    "\n",
    "# Safety config\n",
    "safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfe39e-1fbd-48e3-904b-103b4fb7cb6a",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ac903-dab0-45aa-9f25-ed23e7b5ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_1 = \"\"\"\n",
    "- user: iOS 11 is hot garbage. Ever since my phone updated, my cell data doesnt work about 90% of the time. Might as well be an iPod\n",
    "- support: We're here for you. Have you tried these steps: [link]\n",
    "- user: Appreciate the feedback. Have tried those steps and the phone shows an active lte signal. Just seems to stop data transmission randomly\n",
    "- support: Does this happen with certain apps or usage, or does it happen with all data usage? Do you notice how many bars are showing?\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c82a67-e15a-4f34-967c-bd3235be60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = f\"\"\"\n",
    "Context: You are a helpful, respectful and honest assistant who extracts information from chats between customers and support.\n",
    "    \n",
    "Extract short but descriptive customer issues from the following chat:\n",
    "{chat_1}\n",
    "\n",
    "Provide the issues in JSON format only. Valid fields are 'issue' and 'explanation'.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt_1)\n",
    "# gemini_model.count_tokens(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296db8bd-0774-49c0-87f3-88d6baa8d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_response = gemini_model.generate_content(\n",
    "    prompt_1,\n",
    "    generation_config=gemini_parameters,\n",
    "    safety_settings=safety_config,\n",
    ")\n",
    "print(f\"Response from Model:\\n\\n{gemini_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46170fce-43e9-490a-ae13-5f1205e6741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"Response from Model:\\n\\n{gemini_response.text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3fbac-c34c-4085-a5c6-0e36f09fc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Context: You are a helpful, respectful and honest assistant who extracts information from conversations between customers and support.\n",
    "    \n",
    "Extract short but descriptive customer issues from the following conversation:\n",
    "{chat_2}\n",
    "\n",
    "Provide the issues in JSON format only. Valid fields are 'issue' and 'explanation'.\n",
    "\"\"\"\n",
    "\n",
    "print(prompt_2)\n",
    "# gemini_model.count_tokens(prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd171a-94fd-4263-bda8-7a0a4b95ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_response = gemini_model.generate_content(\n",
    "    prompt_2,\n",
    "    generation_config=gemini_parameters,\n",
    "    safety_settings=safety_config,\n",
    ")\n",
    "print(f\"Response from Model:\\n\\n{gemini_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee79f90-5007-4f0d-acd5-2330aa3e739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Markdown(f\"Response from Model:\\n{gemini_response.text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9763c-946f-4ca7-a692-26f3ebe12086",
   "metadata": {},
   "source": [
    "### Create labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31221e-869c-471a-8027-aa0add019c80",
   "metadata": {},
   "source": [
    "**Task** \n",
    "- We have a group of similar chats that are represented by a set of keywords and we would like to describe these chats by a descriptive label (supplemented by a short description) using LLM.\n",
    "\n",
    "**Problems to solve**\n",
    "- The size of all chats can be larger than the size of the context window\n",
    "- The LLM response should be able to parse using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46353dee-65b7-46f0-b64e-9c12283e80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"helix-ds-metal-dev\", location=\"us-central1\")\n",
    "\n",
    "# Load the model\n",
    "gemini_model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "# Generation config\n",
    "gemini_parameters = {\n",
    "    \"max_output_tokens\": 2048, # default: 8192\n",
    "    \"temperature\": 0.5, # default: 0.9\n",
    "    \"top_p\": 0.8, # default: 1\n",
    "}\n",
    "\n",
    "# Safety config\n",
    "safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d5489-d1eb-47ab-9b05-e36b72dedd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels based on selected chats and existing keywords\n",
    "\n",
    "convs_nb = 100\n",
    "topics = conv_topic_info.loc[1:5, 'Topic']\n",
    "gemini_model_response_labels_list = []\n",
    "\n",
    "for topic_nb in tqdm(topics):    \n",
    "    conv_kyewords = conv_topic_info.loc[topic_nb + 1, 'Representation']\n",
    "    conv_docs = format_selected_conversations(conversations_data_topics, topic_nb, convs_nb, method='highest_score')\n",
    "\n",
    "    label_prompt = f\"\"\"\n",
    "Context: You are a helpful, respectful and honest assistant who extracts information from chats between users and support.\n",
    "\n",
    "I have the following chats:\n",
    "\n",
    "{conv_docs.strip()}\n",
    "\n",
    "These chats are described by the following keywords: {', '.join(conv_kyewords)}.\n",
    "\n",
    "Based on the above information, create a label for these chats with a short and clear description. \n",
    "Provide the label in JSON format only. Valid fields are 'label' and 'description'. Use safe quote nesting if necessary.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        gemini_prompt_response = gemini_model.generate_content(\n",
    "            label_prompt,\n",
    "            generation_config=gemini_parameters,\n",
    "            safety_settings=safety_config,\n",
    "        )\n",
    "        response_text = gemini_prompt_response.text\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        response_text = \"\"\"```json\n",
    "{}\n",
    "```\"\"\"\n",
    "    \n",
    "    gemini_model_response_labels_list.append(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f65c98-c374-40eb-abcd-9bef5a3a943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"Topic {topic_nb}\\nkeywords: {conv_kyewords},\\n{gemini_model.count_tokens(label_prompt)}total characters: {len(label_prompt)}\"\"\")\n",
    "# print(label_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e1818-0fd5-4a2d-8c00-387f2d2e935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show LLM results (in JSON format)\n",
    "\n",
    "for idx, item in enumerate(gemini_model_response_labels_list):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    # print(item)\n",
    "    display(Markdown(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26629a07-f8cb-45f2-ae8c-3a3a10beeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chat examples\n",
    "\n",
    "print(format_selected_conversations(conversations_data_topics, topic_nb=0, nb_of_convs=5, method='highest_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c9d6b-d21a-4c8a-926c-4ef80939eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform LLM results from JSON format to pd.DataFrame\n",
    "\n",
    "df_labels_list = []\n",
    "for idx, item in enumerate(gemini_model_response_labels_list):\n",
    "    label_data = parse_json(item)\n",
    "\n",
    "    df = pd.DataFrame(label_data, index=[0])\n",
    "    df['topic'] = idx\n",
    "    df_labels_list.append(df)\n",
    "    \n",
    "df_labels = pd.concat(df_labels_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f200077-2ba2-4cb9-b8d3-2754c55726b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_labels.set_index('topic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16aec1-ea73-4f2b-95be-3ed9dc258aa7",
   "metadata": {},
   "source": [
    "### Extract issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee140c-3e6b-4df0-b80b-fa86101d50cf",
   "metadata": {},
   "source": [
    "**Task** \n",
    "- We have a group of similar chats that are represented by a set of keywords and we would like to extract the main issues with a short explanation using LLM.\n",
    "\n",
    "**Problems to solve**\n",
    "- The size of all chats can be larger than the size of the context window\n",
    "- The LLM response should be able to parse using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edaef2-8438-4eea-ba74-a444e6354aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"helix-ds-metal-dev\", location=\"us-central1\")\n",
    "\n",
    "# Load the model\n",
    "gemini_model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "\n",
    "# Generation config\n",
    "gemini_parameters = {\n",
    "    \"max_output_tokens\": 2048, # default: 8192\n",
    "    \"temperature\": 0.5, # default: 0.9\n",
    "    \"top_p\": 0.8, # default: 1\n",
    "}\n",
    "\n",
    "# Safety config\n",
    "safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804e3a5-5791-4e86-8cc2-017e06002de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract issues based on selected chats and existing keywords\n",
    "\n",
    "convs_nb = 100\n",
    "topics = conv_topic_info.loc[1:5, 'Topic']\n",
    "gemini_model_response_issues_list = []\n",
    "\n",
    "for topic_nb in tqdm(topics):    \n",
    "    conv_kyewords = conv_topic_info.loc[topic_nb + 1, 'Representation']\n",
    "    conv_docs = format_selected_conversations(conversations_data_topics, topic_nb, convs_nb, method='score_dist')\n",
    "\n",
    "    issue_prompt = f\"\"\"\n",
    "Context: You are a helpful, respectful and honest assistant who extracts information from chats between users and support.\n",
    "\n",
    "I have the following chats:\n",
    "\n",
    "{conv_docs.strip()}\n",
    "\n",
    "These chats are described by the following keywords: {', '.join(conv_kyewords)}.\n",
    "\n",
    "Based on the above information, extract short but highly descriptive issues. Provide up to 3 top issues along with the frequency number of these issues, \n",
    "in JSON format only. Valid fields are 'issue', 'explanation' and 'frequency'. Use safe quote nesting if necessary.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        gemini_prompt_response = gemini_model.generate_content(\n",
    "            issue_prompt,\n",
    "            generation_config=gemini_parameters,\n",
    "            safety_settings=safety_config,\n",
    "        )\n",
    "        response_text = gemini_prompt_response.text\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        response_text = \"\"\"```json\n",
    "[]\n",
    "```\"\"\"\n",
    "    \n",
    "    gemini_model_response_issues_list.append(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b3a83-2658-4ce3-a384-8048df089882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"Topic {topic_nb}\\nkeywords: {conv_kyewords},\\n{gemini_model.count_tokens(issue_prompt)}total characters: {len(issue_prompt)}\"\"\")\n",
    "# print(issue_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca8eb8-f192-4bb4-8144-9c17b8f6ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show LLM results (in JSON format)\n",
    "\n",
    "for idx, item in enumerate(gemini_model_response_issues_list):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    # print(item)\n",
    "    display(Markdown(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882f4c0-1710-411b-ae16-fb92e4661401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chat examples\n",
    "\n",
    "print(format_selected_conversations(conversations_data_topics, topic_nb=1, nb_of_convs=5, method='score_dist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf55ec-c72f-4d7f-9518-db5111ff825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform LLM results from JSON format to pd.DataFrame\n",
    "\n",
    "df_issues_list = []\n",
    "for idx, item in enumerate(gemini_model_response_issues_list):\n",
    "    issue_data = parse_json(item)\n",
    "\n",
    "    df = pd.DataFrame(issue_data)\n",
    "    df['topic'] = idx\n",
    "    df_issues_list.append(df)\n",
    "    \n",
    "df_issues = pd.concat(df_issues_list, ignore_index=True)\n",
    "df_issues['frequency'] /= convs_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207ae61-fb86-4380-80d3-35e4ed6b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_issues.set_index('topic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d454c59-c014-4379-b22c-45e25a278074",
   "metadata": {},
   "source": [
    "### Generate methods to solve issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ffd0d-5766-454c-a4eb-fa8c6047a699",
   "metadata": {},
   "source": [
    "**Task** \n",
    "- We have a group of similar chats that are represented by a set of keywords and we would like to obtain methods to solve the main issue raised in these chats using LLM.\n",
    "\n",
    "**Problems to solve**\n",
    "- The size of all chats can be larger than the size of the context window\n",
    "- The LLM response should be able to parse using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079a396-5215-4c66-bb17-e61883d2813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"helix-ds-metal-dev\", location=\"us-central1\")\n",
    "\n",
    "# Load the model\n",
    "gemini_model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "\n",
    "# Generation config\n",
    "gemini_parameters = {\n",
    "    \"max_output_tokens\": 2048, # default: 8192\n",
    "    \"temperature\": 0.5, # default: 0.9\n",
    "    \"top_p\": 0.3, # default: 1\n",
    "}\n",
    "\n",
    "# Safety config\n",
    "safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc2917-24ae-40ee-9292-722564a7df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain methods to solve the main issue based on selected chats and existing keywords\n",
    "\n",
    "convs_nb = 100\n",
    "topics = conv_topic_info.loc[1:5, 'Topic']\n",
    "gemini_model_response_solve_list = []\n",
    "\n",
    "for topic_nb in tqdm(topics):    \n",
    "    conv_kyewords = conv_topic_info.loc[topic_nb + 1, 'Representation']\n",
    "    conv_docs = format_selected_conversations(conversations_data_topics, topic_nb, convs_nb, method='highest_score')\n",
    "\n",
    "    solve_prompt = f\"\"\"\n",
    "Context: You are a helpful, respectful and honest assistant who extracts information from chats between users and support.\n",
    "\n",
    "I have the following chats:\n",
    "\n",
    "{conv_docs.strip()}\n",
    "\n",
    "These chats are described by the following keywords: {', '.join(conv_kyewords)}.\n",
    "\n",
    "Based on the above information, describe the main issue and generate a list of methods to solve it. Provide the result in JSON format, \n",
    "where valid fields are only 'issue' and 'solution_methods'. Use safe quote nesting if necessary.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        gemini_prompt_response = gemini_model.generate_content(\n",
    "            solve_prompt,\n",
    "            generation_config=gemini_parameters,\n",
    "            safety_settings=safety_config,\n",
    "        )\n",
    "        response_text = gemini_prompt_response.text\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        response_text = \"\"\"```json\n",
    "{}\n",
    "```\"\"\"\n",
    "    \n",
    "    gemini_model_response_solve_list.append(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886752b-6bcb-4a0c-be99-1b930c7ac5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"Topic {topic_nb}\\nkeywords: {conv_kyewords},\\n{gemini_model.count_tokens(solve_prompt)}total characters: {len(solve_prompt)}\"\"\")\n",
    "# print(solve_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bfb96-9417-4a0d-b94d-5323fcd7a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show LLM results (in JSON format)\n",
    "\n",
    "for idx, item in enumerate(gemini_model_response_solve_list):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    # display(Markdown(item))\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f653abc-7012-4ab5-9515-a716b193d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chat examples\n",
    "\n",
    "print(format_selected_conversations(conversations_data_topics, topic_nb=1, nb_of_convs=5, method='highest_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6ceee-dc4a-4e11-8e97-4e8aee3dae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform LLM results from JSON format to pd.DataFrame\n",
    "\n",
    "df_solve_list = []\n",
    "for idx, item in enumerate(gemini_model_response_solve_list):\n",
    "    solve_data = parse_json(item)\n",
    "\n",
    "    df = pd.DataFrame(solve_data)\n",
    "    df['topic'] = idx\n",
    "    df_solve_list.append(df)\n",
    "    \n",
    "df_solves = pd.concat(df_solve_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26396bc1-2e81-47be-9da0-cf873f728746",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_solves.set_index('topic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f1c93-9c44-414d-a06a-e080d8595546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958eb2fb-d0f3-457d-9971-0f1ecd5a8418",
   "metadata": {},
   "source": [
    "### Generate actionable insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca4326-2999-43bf-9df1-6426771aec60",
   "metadata": {},
   "source": [
    "**Task** \n",
    "- We have a group of similar chats that are represented by a set of keywords and we would like to obtain actionable insights about the main issue raised in these chats using LLM.\n",
    "\n",
    "**Problems to solve**\n",
    "- The size of all chats can be larger than the size of the context window\n",
    "- The LLM response should be able to parse using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171434fe-45a3-4e78-af5a-907bd0cbf002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"helix-ds-metal-dev\", location=\"us-central1\")\n",
    "\n",
    "# Load the model\n",
    "gemini_model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
    "\n",
    "# Generation config\n",
    "gemini_parameters = {\n",
    "    \"max_output_tokens\": 2048, # default: 8192\n",
    "    \"temperature\": 0.7, # default: 0.9\n",
    "    \"top_p\": 0.4, # default: 1\n",
    "}\n",
    "\n",
    "# Safety config\n",
    "safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb31cd3-9927-4755-b9df-e5636c7856ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable insights about the main issue based on selected chats and existing keywords\n",
    "\n",
    "convs_nb = 100\n",
    "topics = conv_topic_info.loc[1:5, 'Topic']\n",
    "gemini_model_response_insights_list = []\n",
    "\n",
    "for topic_nb in tqdm(topics):    \n",
    "    conv_kyewords = conv_topic_info.loc[topic_nb + 1, 'Representation']\n",
    "    conv_docs = format_selected_conversations(conversations_data_topics, topic_nb, convs_nb, method='highest_score')\n",
    "\n",
    "    insights_prompt = f\"\"\"\n",
    "Context: You are a helpful, respectful and honest assistant who extracts information from chats between users and support.\n",
    "\n",
    "I have the following chats:\n",
    "\n",
    "{conv_docs.strip()}\n",
    "\n",
    "These chats are described by the following keywords: {', '.join(conv_kyewords)}.\n",
    "\n",
    "Taking into account the above information, generate a list of actionable insights about the main issue that can help decision makers. \n",
    "Provide the result in JSON format, where valid fields are only 'issue' and 'actinable_insights'. Use safe quote nesting if necessary.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        gemini_prompt_response = gemini_model.generate_content(\n",
    "            insights_prompt,\n",
    "            generation_config=gemini_parameters,\n",
    "            safety_settings=safety_config,\n",
    "        )\n",
    "        response_text = gemini_prompt_response.text\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        response_text = \"\"\"```json\n",
    "{}\n",
    "```\"\"\"\n",
    "    \n",
    "    gemini_model_response_insights_list.append(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd86186-71c5-4c61-a91e-c647d486e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"Topic {topic_nb}\\nkeywords: {conv_kyewords},\\n{gemini_model.count_tokens(insights_prompt)}total characters: {len(insights_prompt)}\"\"\")\n",
    "# print(insights_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07601bb-e90d-49ef-ac92-8693dfceeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show LLM results (in JSON format)\n",
    "\n",
    "for idx, item in enumerate(gemini_model_response_insights_list):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    display(Markdown(item))\n",
    "    # print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d6492-897a-4050-afb1-2d70821e94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chat examples\n",
    "\n",
    "print(format_selected_conversations(conversations_data_topics, topic_nb=0, nb_of_convs=5, method='highest_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9de4c-12cc-42a5-9392-35a12b47deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform LLM results from JSON format to pd.DataFrame\n",
    "\n",
    "df_insights_list = []\n",
    "for idx, item in enumerate(gemini_model_response_insights_list):\n",
    "    # print(f\"Topic {idx}\")\n",
    "    insights_data = parse_json(item)\n",
    "\n",
    "    df = pd.DataFrame(insights_data)\n",
    "    df['topic'] = idx\n",
    "    df_insights_list.append(df)\n",
    "\n",
    "df_insights = pd.concat(df_insights_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa63197-2c00-42eb-800d-7ed9d3189c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df_insights.set_index('topic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860ba79-3d39-453e-b0b0-c3447d840175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
